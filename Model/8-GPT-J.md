# GPT-J [EleutherAI] [2021.06] [[Open]](https://github.com/kingoflolz/mesh-transformer-jax)

Blog:[GPT-J-6B: 6B JAX-Based Transformer](https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/)

```yaml
Model Architecture: Decoder Only,GPT Family
Params: 6B
Training Data: The Pile[Text: 730GB Code: 96GB(473B tokens trained)]
Training Time: -
Languages: Multiple
Evaluation: HumanEval
Supported Tasks: Code Generation
```

GPT-J-6B在各种零样本下游任务中的性能几乎与6.7B GPT-3相当。

就各种下游任务的零样本性能而言，GPT-J是性能最好的公开可用Transformer LM。

模型设计和超参数选择与6.7B GPT-3非常相似
