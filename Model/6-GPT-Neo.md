# GPT-Neo [EleutherAI] [2021.03] [[Open]](https://github.com/EleutherAI/gpt-neo)

Paper:GPT-Neo: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow

```yaml
Model Architecture: Decoder Only,GPT Family
Params: 1.3B, 2.7B
Training Data: The Pile[Text: 730GB Code: 96GB(400B tokens trained)]
Training Time: -
Languages: Multiple
Evaluation: HumanEval
Supported Tasks: Code Generation
```

GPT Neo 2.7B是一个使用EleutherAI对GPT-3架构的复制设计的transformer模型。GPT-Neo指的是模型的类别，而2.7B表示该特定预训练模型的参数数量。
