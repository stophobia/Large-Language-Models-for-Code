# GPT-NeoX [EleutherAI] [2022.04] [[Open]](https://github.com/EleutherAI/gpt-neox)

Paper:[GPT-NeoX-20B: An Open-Source Autoregressive Language Model](https://arxiv.org/abs/2204.06745)

```yaml
Model Architecture: Decoder Only,GPT Family
Params: 20B
Training Data: The Pile[Text: 730GB Code: 95GB(473B tokens trained)]
Training Time: -
Languages: Multiple
Evaluation: HumanEval
Supported Tasks: Code Generation
```

GPT-NeoX是目前最大的开源预训练语言模型。模型是在The Pile数据集上训练的，因此它们是在不同领域的自然语言文本和GitHub的源代码上训练的很好的模型代表。
